{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESILV --- Python for Data Analysis --- Devoir 2021\n",
    "## 10 janvier 2021 --- IBO A5 --- Promo 2021\n",
    "\n",
    "<hr>\n",
    "\n",
    "#### Clément Jaccarino et Axel Joly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <hr>\n",
    "# <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset : Faillite d'entreprises polonaises Dataset\n",
    "## Du site de dépôt de l'UCI Machine Learning\n",
    "\n",
    "<hr>\n",
    "\n",
    "#### Créé par Sebastian Tomczak de l'École polytechnique de Wroclaw de Pologne. Les donateurs du dataset sont Sebastian Tomczak, Maciej Zieba et Jakub M. Tomczak. Téléphone : (+48) 7 13 20 44 53. Un article pertinent utilisant ce dataset : \"Ensemble boosted trees with synthetic features generation in application to bankruptcy prediction\" écrit en 2016 par les donateurs ci-dessus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "##### Le dataset concerne la prédiction de la faillite des entreprises polonaises. Les données ont été collectées à partir de l'Emerging Markets Information Service (EMIS), qui est une base de données contenant des informations sur les marchés émergents du monde entier. Les sociétés en faillite ont été analysées sur la période 2000-2012, tandis que les sociétés toujours en activité ont été évaluées de 2007 à 2013.\n",
    "\n",
    "Sur la base des données collectées, cinq cas de classification ont été distingués, qui dépendent de la période de prévision :\n",
    "- 1ère année: les données contiennent les taux financiers de la 1ère année de la période de prévision et le class label correspondant qui indique l'état de faillite après 5 ans. Les données contiennent 7027 instances (états financiers), 271 représentent des entreprises en faillite, 6756 entreprises qui n'ont pas fait faillite au cours de la période de prévision.\n",
    "- 2ème année: les données contiennent les taux financiers à partir de la 2e année de la période de prévision et le class label correspondant qui indique l'état de faillite après 4 ans. Les données contiennent 10 173 instances (états financiers), 400 représentent des entreprises en faillite, 9773 entreprises qui n'ont pas fait faillite au cours de la période de prévision.\n",
    "- 3ème année: les données contiennent les taux financiers à partir de la 3e année de la période de prévision et le class label correspondant qui indique l'état de faillite après 3 ans. Les données contiennent 10 503 instances (états financiers), 495 représentent des entreprises en faillite, 1 0008 entreprises qui n'ont pas fait faillite au cours de la période de prévision.\n",
    "- 4ème année: les données contiennent les taux financiers à partir de la 4e année de la période de prévision et le class label correspondant qui indique le statut de faillite après 2 ans. Les données contiennent 9792 instances (états financiers), 515 représentent des entreprises en faillite, 9277 entreprises qui n'ont pas fait faillite au cours de la période de prévision.\n",
    "- 5ème année: les données contiennent les taux financiers à partir de la 5e année de la période de prévision et le class label correspondant qui indique le statut de faillite après 1 an. Les données contiennent 5910 instances (états financiers), 410 représentent des entreprises en faillite, 5 500 entreprises qui n'ont pas fait faillite au cours de la période de prévision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le dataset se télécharge en tant que dossier compressé comprenant 5 fichiers sous format ARFF\n",
    "# Le format ARFF (Attribute-Relation File Format) décrit une liste d'instances \n",
    "# partageant un ensemble d'attributs en ASCII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On trouve des outils permettant de basculer du format ARFF vers CSV.\n",
    "# L'outil que l'on a utilisé : ARFF2CSV (https://pulipulichen.github.io/jieba-js/weka/arff2csv/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des 5 fichiers CSV et première visualisation\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "\n",
    "type_colonne ={\"hrmn\" : str }\n",
    "df_og_1 = pd.read_csv(\"final_project_data/csv_result-1year.csv\")\n",
    "df_og_2 = pd.read_csv(\"final_project_data/csv_result-2year.csv\")\n",
    "df_og_3 = pd.read_csv(\"final_project_data/csv_result-3year.csv\")\n",
    "df_og_4 = pd.read_csv(\"final_project_data/csv_result-4year.csv\")\n",
    "df_og_5 = pd.read_csv(\"final_project_data/csv_result-5year.csv\")\n",
    "df_og_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concaténation des DataFrames tout en gardant les caractéristiques de chaque DataFrame (X année)\n",
    "\n",
    "dataframes = [df_og_1, df_og_2, df_og_3, df_og_4, df_og_5]\n",
    "\n",
    "conc_dataframes  = pd.concat(dataframes, keys=[\"1st year\", \"2nd year\", \"3rd year\", \"4th year\", \"5th year\"])\n",
    "conc_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification que la concaténation a bien marchée\n",
    "\n",
    "nb_lignes, nb_col = conc_dataframes.shape\n",
    "print(nb_lignes)\n",
    "print(nb_lignes == (7027 + 10173 + 10503 + 9792 + 5910))\n",
    "print(nb_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des colonnes originales\n",
    "\n",
    "conc_dataframes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouvelles colonnes d'après lecture du descriptif de l'UCI et confirmation du changement effectué\n",
    "\n",
    "nouvelles_colonnes = [\n",
    "    \"id\",\n",
    "    \"net profit / total assets\",\n",
    "    \"total liabilities / total assets\",\n",
    "    \"working capital / total assets\",\n",
    "    \"current assets / short-term liabilities\",\n",
    "    \"[(cash + short-term securities + receivables - short-term liabilities) / (operating expenses - depreciation)] * 365\",\n",
    "    \"retained earnings / total assets\",\n",
    "    \"EBIT / total assets\",\n",
    "    \"book value of equity / total liabilities\",\n",
    "    \"sales / total assets\",\n",
    "    \"equity / total assets\",\n",
    "    \"(gross profit + extraordinary items + financial expenses) / total assets\",\n",
    "    \"gross profit / short-term liabilities\",\n",
    "    \"(gross profit + depreciation) / sales\",\n",
    "    \"(gross profit + interest) / total assets\",\n",
    "    \"(total liabilities * 365) / (gross profit + depreciation)\",\n",
    "    \"(gross profit + depreciation) / total liabilities\",\n",
    "    \"total assets / total liabilities\",\n",
    "    \"gross profit / total assets\",\n",
    "    \"gross profit / sales\",\n",
    "    \"(inventory * 365) / sales\",\n",
    "    \"sales (n) / sales (n-1)\",\n",
    "    \"profit on operating activities / total assets\",\n",
    "    \"net profit / sales\",\n",
    "    \"gross profit (in 3 years) / total assets\",\n",
    "    \"(equity - share capital) / total assets\",\n",
    "    \"(net profit + depreciation) / total liabilities\",\n",
    "    \"profit on operating activities / financial expenses\",\n",
    "    \"working capital / fixed assets\",\n",
    "    \"logarithm of total assets\",\n",
    "    \"(total liabilities - cash) / sales\",\n",
    "    \"(gross profit + interest) / sales\",\n",
    "    \"(current liabilities * 365) / cost of products sold\",\n",
    "    \"operating expenses / short-term liabilities\",\n",
    "    \"operating expenses / total liabilities\",\n",
    "    \"profit on sales / total assets\",\n",
    "    \"total sales / total assets\",\n",
    "    \"(current assets - inventories) / long-term liabilities\",\n",
    "    \"constant capital / total assets\",\n",
    "    \"profit on sales / sales\",\n",
    "    \"(current assets - inventory - receivables) / short-term liabilities\",\n",
    "    \"total liabilities / ((profit on operating activities + depreciation) * (12/365))\",\n",
    "    \"profit on operating activities / sales\",\n",
    "    \"rotation receivables + inventory turnover in days\",\n",
    "    \"(receivables * 365) / sales\",\n",
    "    \"net profit / inventory\",\n",
    "    \"(current assets - inventory) / short-term liabilities\",\n",
    "    \"(inventory * 365) / cost of products sold\",\n",
    "    \"EBITDA (profit on operating activities - depreciation) / total assets\",\n",
    "    \"EBITDA (profit on operating activities - depreciation) / sales\",\n",
    "    \"current assets / total liabilities\",\n",
    "    \"short-term liabilities / total assets\",\n",
    "    \"(short-term liabilities * 365) / cost of products sold)\",\n",
    "    \"equity / fixed assets\",\n",
    "    \"constant capital / fixed assets\",\n",
    "    \"working capital\",\n",
    "    \"(sales - cost of products sold) / sales\",\n",
    "    \"(current assets - inventory - short-term liabilities) / (sales - gross profit - depreciation)\",\n",
    "    \"total costs /total sales\",\n",
    "    \"long-term liabilities / equity\",\n",
    "    \"sales / inventory\",\n",
    "    \"sales / receivables\",\n",
    "    \"(short-term liabilities *365) / sales\",\n",
    "    \"sales / short-term liabilities\",\n",
    "    \"sales / fixed assets\",\n",
    "    \"Class\"\n",
    "]\n",
    "\n",
    "conc_dataframes.columns = nouvelles_colonnes\n",
    "df_og_1.columns = nouvelles_colonnes\n",
    "df_og_2.columns = nouvelles_colonnes\n",
    "df_og_3.columns = nouvelles_colonnes\n",
    "df_og_4.columns = nouvelles_colonnes\n",
    "df_og_5.columns = nouvelles_colonnes\n",
    "conc_dataframes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des types de chaque attribut\n",
    "\n",
    "conc_dataframes.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tous les attributs étant de type numéric (flottant) d'après l'en-tête de chaque fichier ARFF\n",
    "# Il faut modifier cela. Mais avant, vérifions s'il y a des NaNs\n",
    "\n",
    "while conc_dataframes.isnull().values.any():\n",
    "    conc_dataframes.replace(True, 0)\n",
    "    conc_dataframes.replace(np.nan, 0)\n",
    "    where_are_NaNs = np.isnan(conc_dataframes)\n",
    "    conc_dataframes[where_are_NaNs] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modification du type de chaque attribut vers float\n",
    "\n",
    "conc_dataframes = conc_dataframes.apply(pd.to_numeric, errors='coerce')\n",
    "conc_dataframes.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_og_1 = df_og_1.apply(pd.to_numeric, errors='coerce')\n",
    "df_og_2 = df_og_2.apply(pd.to_numeric, errors='coerce')\n",
    "df_og_3 = df_og_3.apply(pd.to_numeric, errors='coerce')\n",
    "df_og_4 = df_og_4.apply(pd.to_numeric, errors='coerce')\n",
    "df_og_5 = df_og_5.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "meanYear1Company_0 = df_og_1[(df_og_1['Class']==0)][\"net profit / total assets\"].mean()\n",
    "meanYear1Company_1 = df_og_1[(df_og_1['Class']==1)][\"net profit / total assets\"].mean()\n",
    "\n",
    "meanYear2Company_0 = df_og_2[(df_og_2['Class']==0)][\"net profit / total assets\"].mean()\n",
    "meanYear2Company_1 = df_og_2[(df_og_2['Class']==1)][\"net profit / total assets\"].mean()\n",
    "\n",
    "meanYear3Company_0 = df_og_3[(df_og_3['Class']==0)][\"net profit / total assets\"].mean()\n",
    "meanYear3Company_1 = df_og_3[(df_og_3['Class']==1)][\"net profit / total assets\"].mean()\n",
    "\n",
    "meanYear4Company_0 = df_og_4[(df_og_4['Class']==0)][\"net profit / total assets\"].mean()\n",
    "meanYear4Company_1 = df_og_4[(df_og_4['Class']==1)][\"net profit / total assets\"].mean()\n",
    "\n",
    "meanYear5Company_0 = df_og_5[(df_og_5['Class']==0)][\"net profit / total assets\"].mean()\n",
    "meanYear5Company_1 = df_og_5[(df_og_5['Class']==1)][\"net profit / total assets\"].mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def normalize(dataset):\n",
    "    dataNorm=((dataset-dataset.mean())/dataset.std())\n",
    "    dataNorm[\"Class\"]=dataset[\"Class\"]\n",
    "    dataNorm[\"id\"]=dataset[\"id\"]\n",
    "    return dataNorm\n",
    "\n",
    "# Remove NaN\n",
    "if df_og_1.isnull().values.any():\n",
    "    df_og_1.dropna(inplace=True)\n",
    "df_og_1[df_og_1['Class']==1]\n",
    "\n",
    "# Remove outliers\n",
    "z = np.abs(stats.zscore(df_og_1[\"net profit / total assets\"]))\n",
    "dataset_cleared_zscore = df_og_1[(z < 3)]\n",
    "\n",
    "sns.relplot(x=\"net profit / total assets\", y=\"working capital / total assets\", data=dataset_cleared_zscore, hue=\"Class\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_og_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(conc_dataframes.corr(method='pearson'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "corr = conc_dataframes.corr(method='pearson')\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson = conc_dataframes.corr(method='pearson')\n",
    "corr_with_target = pearson.iloc[-1,:-1]\n",
    "corr_with_target = corr_with_target[1:]\n",
    "corr_with_target.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_with_target[abs(corr_with_target).argsort()[::-1]][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributs = pearson.iloc[:-1,:-1]\n",
    "\n",
    "threshold = 0.5\n",
    "important_correlations = (attributs[abs(attributs) > threshold][attributs != 1.0]) \\\n",
    "    .unstack().dropna().to_dict()\n",
    "\n",
    "unique_important_correlations = pd.DataFrame(\n",
    "    list(set([(tuple(sorted(key)), important_correlations[key]) \\\n",
    "    for key in important_correlations])), columns=['attribute pair', 'correlation'])\n",
    "\n",
    "unique_important_correlations = unique_important_correlations.iloc[\n",
    "    abs(unique_important_correlations['correlation']).argsort()[::-1]]\n",
    "\n",
    "pd.set_option('max_colwidth', 400)\n",
    "unique_important_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "correlation = conc_dataframes.corr()\n",
    "\n",
    "invisible = np.zeros_like(correlation, dtype=np.bool)\n",
    "invisible[np.triu_indices_from(invisible)] = True\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "vmax = np.abs(correlation.values[~invisible]).max()\n",
    "sns.heatmap(correlation, mask=invisible, cmap=plt.cm.PuOr, vmin=-vmax, vmax=vmax,\n",
    "            square=True, linecolor=\"lightgray\", linewidths=1, ax=ax)\n",
    "for i in range(len(correlation)):\n",
    "    for j in range(i+1, len(correlation)):\n",
    "        s = \"{:.3f}\".format(correlation.values[i,j])\n",
    "#ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning\n",
    "# Supervised learning : classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc_dataframes.replace(True, 0)\n",
    "conc_dataframes.replace(np.nan, 0)\n",
    "where_are_NaNs = np.isnan(conc_dataframes)\n",
    "conc_dataframes[where_are_NaNs] = 0\n",
    "conc_dataframes.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation jeu de données, d'abord en jeu de features/target, puis en jeu de train/test\n",
    "\n",
    "X = conc_dataframes.iloc[:,:-1]\n",
    "y = conc_dataframes.iloc[:,-1:]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On standardise les valeurs\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On choisit un premier modèle. Comme nous avons des échantillons appartenant à deux classes\n",
    "# (0 = pas en faillite, 1 = en faillite), on choisit un premier modèle de type classification\n",
    "# comme une régression logistique\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression()\n",
    "modele = logisticRegr.fit(X_train, y_train)\n",
    "modele.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  cross_val_score\n",
    "cross_val_score(logisticRegr, X, y, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On choisit un second modèle. Comme nous avons des échantillons appartenant à deux classes\n",
    "# (0 = pas en faillite, 1 = en faillite), on choisit un second modèle de type classification\n",
    "# comme SVC\n",
    "\n",
    "from sklearn import svm\n",
    "svc = svm.SVC()\n",
    "modele2 = svc.fit(X_train, y_train)\n",
    "modele2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  cross_val_score\n",
    "cross_val_score(svc, X, y, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On choisit un second modèle. Comme nous avons des échantillons appartenant à deux classes\n",
    "# (0 = pas en faillite, 1 = en faillite), on choisit un second modèle de type classification\n",
    "# comme Bernoulli Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "gnb = BernoulliNB()\n",
    "modele2 = gnb.fit(X_train, y_train)\n",
    "modele2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  cross_val_score\n",
    "cross_val_score(gnb, X, y, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut commencer à tuner les paramètres des trois modèles précédents\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def test_hyperparametres(hyperparametres, modele):\n",
    "    grid = GridSearchCV(modele, hyperparametres, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print (grid.best_score_, grid.best_estimator_)    \n",
    "    return grid.best_score_, grid.best_estimator_\n",
    "                    \n",
    "hyperparametres = {'kernel' : ['linear', 'sigmoid'],\n",
    "                   'C' : [1],\n",
    "                   'gamma' : [0.1, 0.5]\n",
    "                  }\n",
    "\n",
    "test_hyperparametres(hyperparametres, svm.SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparametres = {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
    "                   'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "                   'C' : [1, 5, 10]\n",
    "                  }\n",
    "\n",
    "test_hyperparametres(hyperparametres, LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparametres = {'alpha' : [1.0, 0.5, 0.25, 0.1, 0.01, 0.0],\n",
    "                   'binarize' : [0.0, 0.01, 0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "                  }\n",
    "\n",
    "test_hyperparametres(hyperparametres, BernoulliNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Django"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame concaténé vers un fichier csv\n",
    "conc_dataframes.to_csv('dataset_django.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
